:PROPERTIES:
:ID:       f009f5b6-ec4d-4449-b91e-54397f3ab822
:END:
#+title: [Book] Security Engineering
#+bibliography: "../bib/security-engineering-3rd.bib"

[[https://github.com/64J0/Emacs-config/blob/master/org/security-engineering-3rd-book.org][Repository on github]].

* Chapter 6 - Access Control

[[id:273edbf4-0d24-45f6-bd13-a8fadfbb6a15][Access Control]]

* Chapter 7 - Distributed Systems

[[id:8994353b-0aaf-441f-b88d-ae46f37714f0][[Security] Distributed systems]]

* Chapter 8 - Economics

[[id:7972dc99-50c7-4c2e-946c-df7fa41c3154][[Security] Economics]]

* Chapter 9 - Multilevel Security

[[id:6cc42aac-f451-4f5d-bcf6-c1d33c0d0118][[Security] Multilevel security]]

* Chapter 10 - Boundaries

[[id:7f7ccf26-37e9-46a5-aa0c-b5fcd38fb4fd][[Security] Boundaries]]

* Chapter 11 - Inference Control

  Just as Big Tobacco spent decades denying that smoking causes lung cancer, and
  Big Oil spent decades denying climate change, so also Big Data has spent
  decades pretending that sensitive personal data can easily be 'anonymised' so
  it can be used as an industrial raw material without infriging on the privacy
  rights of the data subjects.

  Anonymisation is an aspirational term that means stripping identifying
  information from data in such a way that useful statistical research can be
  done without leaking information about identifiable data subjects.

  [...]

  Since 2006, we have a solid theory of exactly how much protection we can get
  from adding randomness: differential privacy.

  [...]

  By 2011 Google was describing its core competence as 'the statistical data
  mining of crowdsourced data'; as the datasets got larger, and basic
  statistical techniques were augmented with machine learning, the amount we can
  learn has grown.

  [...]

  So is it possible to do anonymisation properly? The answer is yes; in certain
  circumstances, it is. Although it is not possible to create anonymous datasets
  that can be used to answer any question, we can sometimes provide a dependable
  measure of privacy when we set out to answer a specific set of reasearch
  questions. This brings us to the theory of differential privacy.

** 11.3 - Differential privacy

   In 2006, Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith published
   a seminal paper showing how you could systematically analyse privacy systems
   that added noise to prevent disclosure of sensitive statistics in a
   database. (“Calibrating noise to sensitivity in private data analysis”, Third
   conference on Theory of Cryptography (2006)). Their theory, /differential
   privacy/, enables the security engineer to limit the probability of
   disclosure, even in the presence of an adversary with unbounded computational
   power and copious side information, and can thus be seen as the equivalent of
   the one-time pad and unconditionally secure authentication codes in
   cryptography.

   + Gold standard for both statistical database security and for anonymisation
     in general.

   [...]

   There is now a growing research literature exploring how such mechanisms can
   be extended for static to dynamic databases, to data streams, to mechanism
   design and to machine learning. But can the promise of learning nothing
   useful about individuals while learning useful information about a
   population, be realised in practical applications?

   Differential privacy is now getting a full-scale test in the 2020 US
   census. The census is not allowed to publish anything that identifies the
   data of any individual or establishment; collected data must by law be kept
   confidential for 72 years and used only for statistical purposes until then.

** 11.4 - Mind the gap?

   Firms want to be responsible, but how do you give live data to your
   development and test teams? How can you collaborate with academics and
   startups? How can you sell data products? Anonymisation technologyu is all
   pretty rudimentary at this scale, and as you just don't know what's goingo
   on, it's beyong the scope of differential privacy or anything else you can
   analyse cleanly. You can tokenise the data on ingest to get rid of the
   obvious names, then control access and use special tricks for time series and
   location streams, but noise addition doesn't work on trajectories and there
   are lots of creative ways to re-identify location data (e.g., photos of
   celebs getting in and out of taxis). This get even harder where people are
   partially authorised and have partial access.

   Future problems may come from AI and machine learning; that's the fashion
   now, following the 'Big Data' fashion of the mid-2010s that led firms to set
   up large data warehouses. You're now training up systems that generally can't
   explain what they do, on data you don't really understand. We already know of
   lots of things that can go wrong. Insurance systems jack up premiums in
   minority neighbourhoods, breaking anti-discrimination laws. And machine
   learning systems inhale existing social prejudices along with their training
   data; as machine-translation systems read gigabytes of online text, they
   become much better at translation but they also become racist, sexist and
   homophobic. Another problem is that if a neural network is trained on
   personal data, then it will often be able to identify some of those persons
   if it comes across them again - so you can't just train it and then release
   it in the hope that its knowledge is somehow anonymous, as we might hope for
   averages derived from large aggregates of data.

* Chapter 12 - Banking and Bookkeeping

  #+begin_comment
    bookkeeping = contabilidade.
  #+end_comment
  
  A banking system should prevent customers from cheating on each other, or the
  bank; it should prevent bank staff from cheating the bank, or its customers;
  and the evidence it provides should be good enough that none of them can get
  away with falsely accusing others of cheating. Banking and bookkeeping
  pioneered the use of dual control, also known nowadays as multi-party
  authorisation. [...] This kind of system gives us a well-understood model of
  protection in which confidentiality plays little role, but where the integrity
  of records (and their immutability once made) is paramount.

*** Double-entry bookkeeping

    The idea behind double-entry bookkeeping is simple: each transaction is
    posted to two separate books, as a credit in one and a debit in the
    other. [...] At the end of the day, the books should /balance/, that is, add
    up to zero; the assets and the liabilities should be equal. In all but the
    smallest firms, the books were kept by different clerks.

    We arrange things so that each branch can be balanced separately. Each
    cashier will balance their cash tray before locking it in the vault
    overnight; the debits in the cash ledger should exactly balance the physical
    banknotes they've collected. So most frauds need the collusion of two or
    more people, and this principle of split responsibility, also known as dual
    control or multi-party authorisation (MPA), is complemented by audit. Not
    only are the books audited at year end, but there are random audits too;
    inspectors may descend on a branch at no notice and insist that all the
    books are balanced before the staff go home.

*** The Clark-Wilson security policy model

    In this model, some data items are constrained so that they can only be
    acted on by a certain set of transformation procedures.

    + UDI: Unconstrained data item
    + CDI: Constrained data item
    + IVP: Integrity verification procedures
    + TP: Transformation procedures

    ---
    
    1. The system will have an IVP for validating the integrity of any CDI;
    2. The application of a TP to any CDI must maintain its integrity;
    3. A CDI can only be changed by a TP;
    4. Subjects can only initiate certain TPs on certain CDIs;
    5. Triples must enforce an appropriate separatio-of-duty policy on
       subjects;
    6. Certain special TPs on UDIs can produce CDIs as output;
    7. Each application of a TP must cause enough information to reconstruct
       it to be written to a special append-only CDI;
    8. The system must authenticate subjects attempting to initiate a TP;
    9. The system must let only special subjects (i.e., security officers)
       make changes to authorization-related lists.

    [...]

    The hard question remains, namely: how do we control the risks from
    dishonest staff? (rule 5).

    [...]
    
    What happens in practice is that the big four accountancy firms have a list
    of controls that they push to their audit clients - a typical company may
    have a checklist of about 300 internal controls that it has to maintain,
    depending on what sector it's in.

*** Designing internal controls

    [...] Self-regulation failed to stop the excesses of the dotcom era, and
    following the collapse of Enron there was intervention from US lawmakers in
    the form of the /Sarbanes-Oxley Act/ (SOX) of 2002. SOX regulates all US
    public companies, making senior executives responsible for the accuracy and
    completeness of financial reports, whose truthfulness CEOs have to certify;
    protecting whistleblowers, who are the main source of information on insider
    fraud; and making managers responsible for maintaining "adequate internal
    control structure and procedures for financial reporting". It also demands
    that auditors disclose any "material weaknesses". Most of the compliance
    cost of SOX are reckoned to come from internal controls. 

*** Finding the weak spots

    If you are ever responsible for security in an organisation, you should not
    just think about which components might, by their failure, cause a bad
    enough loss to make a material difference to the bottom line. You need to
    think about the people too, and their external relationships.

    + Which of your managers could defraud your company by colluding with
      customers or suppliers?
    + Could a branch manager be lending money to a dodgy business run by his
      cousin against forged collateral?
    + Could he have sold life-insurance policies to nonexistent people and
      forged their death certificates?
    + Could an operations manager be taking bribes from a supplier?
    + Could your call-centre staff be selling data from the accounts they've
      dealt with to a phishing gang who use this data to impersonate your
      company to your customers?

    ---
    
    Lessons:
    
    According to statistical studies, 1% of staff fall into temptation every
    year.

    A trusted person is one who can damage you.
      
    ---

** Credit cards

   [...]

   When you use a credit card to pay for a purchase in a store, the transaction
   flows from the merchant to their bank (the acquiring bank), which pays them
   after deducting a merchant discount of typically just under 2% for a small
   merchant. If the card was issued by a different bank, the transaction now
   flows to a switch such as VISA, which passes it to the issuing bank for
   payment. Each transaction involves two components: authorisation, when you
   present your card at a merchant and they want to know right now whether to
   give you the goods, and settlement, which flows through a separate system and
   gets money to the merchant, often two or three days later. The issuer also
   gets a slice of the merchant discount, but makes most of its money from
   extending credit to cardholders.
   
*** Fraud engines

    [...]

    The core of a good fraud engine tends to be several dozen signals extracted
    from the transaction stream on the basis of a set of well-understood threat
    vectors (such as bad IP addresses, or too many logons from the same IP
    address) and a set of quality signals (such as 'card old but good'). These
    signals are then fed to a machine-learning system that scores the
    transactions. The signals appear to be the most important part of the
    design, not whether you use an SVM or a Bayesian network. The signals need
    to be continuously curated and updated as the bad guys learn new tricks, and
    the fraud engine needs to be well integrated with the human processes. As
    for how engines fail, the regulator's report into a 2016 fraud against Tesco
    Bank found that the staff failed to 'exercise due skill, case and
    dilligence' over the fraud detection rules, and to 'respond to the attack
    with sufficient rigor, skill and urgency'. In that case, the bank failed to
    update its fraud engine following a warning from Mastercard the previous day
    of a new type of card scam.

* Chapter 13 - Locks and Alarms

  Most security engineers nowadays focus on electronic systems, but physical
  protection cannot be neglected.

  First, if you're advising on a company's overall risk management, then walls
  and locks are a factor.

  Second, as it's easier to teach someone with an electrical engineering or
  computer science background the basics of physical security than the other way
  round, interactions between physical and logical protection are usually up to
  the systems person to manage.

  Third, you will often be asked for your opinion on your client's
  installations - which may have been built by contractors with little
  understanding of system issues. You'll need to be able to give informed, but
  diplomatic, advice.

  Fourth, many information security mechanisms can be defeated if a bad man gets
  physical access, whether at the factory, or during shipment, or before
  installation.

  Fifth, many mechanical locks have recently been completely compromised by
  'bumping', an easy covert-entry technique; their manufactures often seem
  unaware of vulnerabilities that enable their products to be quickly bypassed.

  Finally, many of the electronic locks that are replacing them are easy to
  compromise, either because they use cryptography that's broken (such as Mifare
  classic) or because of poor integration of the mechanical and digital
  components.

** Threats and barriers

   [...] The design and testing of entry controls and alarms are driven by a
   policy based on:

   + Deter - detect - alarm - delay - respond

*** Deterrence

    The first consideration is whether you can prevent bad people from ever
    trying to break in. In this regard, it's a good idea to make your asset
    anonymous and inconspicuous if you can.

    Location matters; some neighbourhoods have much less crime than others. Part
    of this has to do with whether property nearby is protected, and how easy it
    is for a crook to tell which properties are protected.

    If owners just install visible alarms, they may redistribute crime to their
    neighbours; but invisible alarms that get criminals caught rather than just
    sent next door can deter crime in a whole neighbourhood.

    For example, Ian Ayres and Steven Levitt studied the effect on auto thefts
    of Lojack, a radio tag that's embedded invisibly in cars and lets the police
    find them if they're stolen. In tows where a lot of cars have Lojack, car
    thieves are caught quickly, and 'chop-shops' that break up stole cars for
    parts are closed down. [...] The same applies to real estate; a
    neighbourhood in which lots of houses have high-grade alarms that quietly
    call the police is a dangerous place for a burglar to work.

**** Situational crime prevention

     + Increase the risks and efforts
     + Reduce the rewards and provocations
     + Remove excuses

** Alarms

   Alarms are used to deal with much more than burglary. Their applications
   range from monitoring freezer temperatures in supermarkets (so staff don't
   'accidentally' switch off freezer cabinets in the hope of being given food to
   take home), right through to improvised explosive devices in conflict zones
   that are often booby-trapped.

   [...]

   The /Titanic Effect/ of over-reliance on the latest technology often blinds
   people to common sense.

* Chapter 14 - Monitoring and Metering

  No notes for this chapter.

* Chapter 15 - Nuclear Command and Control
* Chapter 17 - Biometrics

  Biometrics identify people by measuring some aspect of individual anatomy or
  physiology (such as your hand geometry or fingerprint), some deeply ingrained
  skill or behavior (such as your handwritten signature), or some combination of
  the two (such as your voice).

  [...]

  The biometric systems market has taken off like a rocket, growing fro $50m in
  1998 to over $1.5bn in 2005 and $33bn in 2019.

  [...]

  In most of the English-speaking world, most documents do not need to be
  authenticated by special measures. The essence of a signature is the intent of
  the signer, so an illiterate’s ‘X’ on a document is perfectly valid. A
  plaintext name at the bottom of an email message therefore has full legal
  force, except where there are specific regulations to the contrary.

  [...]

  Like alarms, most biometric systems have a tradeoff between false-accept and
  false-reject rates, often referred to in the banking industry as the fraud and
  insult rates and in the biometric literature as type 1 and type 2 errors.

  [...]

  Recognizing people by the patterns in the irises of their eyes has far and
  away the best error rates of any automated biometric system when measured
  under lab conditions.

* Chapter 18 - Tamper Resistance

The best cryptographic modules used in banking and government withstand all
known types of physical attack, and can only be defeated when people either run
insecure software on them or rely on inse- cure devices to interface with
users. Smartcard tamper resistance has evolved in a long war between pay-TV
pirates cloning subscriber cards and the pay-TV industry trying to stop them,
and was honed in an arms race between firms that wanted to lock down their
products, and others who wanted to unlock them.  The tussles over printer
cartridges were important here, as both the printer makers who were trying to
control aftermarkets, and the independent car- tridge makers who were trying to
break into these markets, are acting lawfully.  Other hackers work for lawyers,
reverse engineering products to prove patent infringements. There are academics
who hack systems for glory, and to push forward the state of the art. And
finally there are lots of grey areas. If you find a way to unlock a mobile
phone, so that it can be used on any network, is that a crime? It depends on how
you do it, and on what country you’re in.

+ Our subject here is the physical defenses against tampering.

** IBM 4758 cryptoprocessor

It was the first commercial product to be evaluated to the highest level of
tamper resistance (FIPS 140-1 level 4) [1401] then set by the US
government. Second, there is an extensive literature about it, including its
history, hardware and software. Third, it was therefore a high-profile target,
and from 2000–2005 my students and I put a lot of effort into attacking it and
understanding the residual vulnerabilities. Fourth, the current IBM flagship
product, the 4765, isn’t hugely changed except for fixing some of the bugs we
found.

** HSM - Hardware Security Modules

These are micro-computers encased in robust metal enclosures, with encryption
hardware and special key memory, static RAM that is zeroized when the enclosure
is opened.  Initially, this just involved wiring the power supply to the key
memory through a number of lid switches. So whenever the maintenance crew came
to replace batteries, they’d open the lid and destroy the keys. Once they’d
finished, the HSM custodians would reload the key material. In this way, the
HSM’s owner could hope that its keys were under the unique control of its own
trustwor- thy staff.

* Chapter 19 - Side Channels

  ~Fire answers fire.~

Side channel attacks are everywhere, and 3-4 of them have caused multi-billion
dollar losses.

1. First, there are conducted or radiated electromagnetic signals,
   which can compromise information locally and occasionally at
   longer ranges. These ‘Tempest’ attacks led NATO governments
   to spend billions of dollars a year on shielding equipment, start-
   ing in the 1960s. After the end of the Cold War, people started
   to realise that there had usually been nobody listening.
2. Second, side channels leak data between tasks on a single device, or
   between devices that are closely coupled; these can exploit both power
   and timing information, and also contention for shared system resources.
   The discovery of Differential Power Analysis in the late 1990s held up
   the deployment of smartcards in banking and elsewhere by 2–3 years
   once it was realised that all the cards then on sale were vulnerable.
3. The third multibillion-dollar incident started in January 2018 with
   the announcement of the ‘Spectre’ and ‘Meltdown’ attacks, which
   exploit speculative execution to enable one process on a CPU to snoop
   on another, for example to steal its cryptographic keys. This will
   probably force the redesign of all superscalar CPUs over 2020.
4. There are attacks that exploit shared local physical resources, such as
   when a phone listens to keystrokes entered on a nearby keyboard, or
   indeed on a keyboard on its own touch screen – whether that sensing is
   done with microphones, the accelerometer and gyro, or even the camera.
   Another example is that a laser pulse can create a click on a microphone,
   so a voice command can be given to a home assistant through a window.
   So far, none of the side-channel attacks on phones and other IoT devices
   has scaled up to have major impact – but there are ever more of them.
5. Finally, there are attacks that exploit shared social resources. An
   example is identifying someone in a supposedly anonymous
   dataset from patterns of communications, location history or even
   just knowing when they went on holiday. This has led to many
   poor policy decisions and much wishful thinking around whether
   personal data can be anonymised sufficiently to escape privacy
   law. There have been both scandalous data leaks, and complaints
   that data should be made more available for research and other
   uses. It’s hard to put a dollar value on this, but it is significant in
   fields such as medical research.

* Chapter 20 - Advanced Cryptographic Engineering

** Full-disk encryption

The idea behind full-disk encryption (FDE) is simple. You encrypt data as it’s
written to disk, and do decryption as it’s read again. The key depends on an
initial authentication step such as a password, which is forgotten when the
machine sleeps or is switched off. So if a doctor leaves their laptop on a
train, only the hardware is lost; the medical records are not. FDE has become a
regulatory requirement in many industries. In Europe, privacy regulators
generally see the loss of machines with FDE as not serious enough to attract a
fine or to need mandatory notification of data subjects. Many phones and laptops
come with FDE; with some it’s enabled by default (Android) while with others it
just takes a click (Mac).

** Signal

Signal is a free messaging app, initially developed by a man who uses the name
of Moxie Marlinspike. It set the standard for end-to-end encryption of
messaging, and its mechanisms have been adopted by competing products including
WhatsApp. Mobile messages can be highly sensitive, with everything from lovers’
assignations through business deals to political intrigues at diplomatic
summits; yet mobile phones are often lost or stolen, or sent in for repair when
the screens break. So key material in phones is frequently exposed to
compromise, and it’s not enough to just have a single long-lived private key in
an app. The Signal protocol therefore provides the properties of forward
secrecy, that a key compromise today won’t expose any future traffic, and back-
ward secrecy, which means that it won’t expose previous traffic either. These
are now formalised as post-compromise security.

The protocol has three main components: the Extended Triple Diffie-Hellman
(X3DH) protocol to set up keys between Alice, Bob and the server; a ratchet
protocol to derive message keys once a secret key is established; and mechanisms
for finding the Signal keys of other people in your address book.

(...)

Previous attempts to help ordinary people use end-to-end encryption, such as the
email encryption program PGP, never got much traction outside specialist niches
because key management was too much bother. Messaging apps solved the usability
problem by demanding access to your address book, looking up all your contacts
on their servers to see who else was a user and then flagging them so you know
you can message them. However, giving service firms a copy of your address book
is already a privacy compromise, and if you also let them keep a plaintext
record of your social graph, profile name, location, group memberships and who
is messaging whom, then investigators can get all this by subpoena.

** Tor

The Onion Router (Tor) is the main system people use to get serious anonymity
online, with about 2 million concurrent users in 2020. It began its life in 1998
at the US Naval Research Laboratory, and was called Onion Routing because
messages in it are nested like the layers of an onion. If Alice wants to visit
Eve’s website without Eve or anyone else being able to identify her, she sets up
a TLS connection to a Tor relay operated by Bob, which sets up a TLS connection
to a Tor relay operated by Carol, which in turn sets up a TLS connection to a
Tor relay operated by David – from whose ‘exit node’ Alice can now establish a
connection to Eve’s website. The idea is to separate routing from identity –
anyone wanting to link Alice to Eve has to subvert Bob, Carol and Dave, or
monitor the traffic in and out of Bob’s and David’s systems.

* Chapter 21 - Network Attack and Defence
* Chapter 25 - New Directions?

Ross Anderson security group runs a blog: www.lightbluetouchpaper.org.

In this chapter the author is going to discuss *four* classes of application at
the bleeding edge of security research.

** Autonomous Cars

Why it's hard? People driving other cars on the same road. Other road users are
*unpredictable*.

Autonomous cars nowadays rely in the safety driver to get the control in case of
problems, but we’ve known for decades that relying on humans to take over in an
emergency takes time:

1. a human has to react to an alarm,
2. analyse the alarm display on the console,
3. scan the environment,
4. acquire situational awareness,
5. get into the optical flow,
6. and take effective control.

Even in commercial aviation, it takes a flight crew about *eight seconds* to
regain control properly after an autopilot failure. You cannot expect a safety
driver in a car to do much better.

*** Levels and limits of automation

1. Driver assistance - the software controls either steering or speed, and the
   human driver does the rest of the work;
2. Partial automation - the software controls both steering and speed in some
   modes but the human driver is responsible for monitoring the environment and
   assuming control at zero notice if the software gets confused;
3. Conditional automation - the software monitors the environment, and controls
   both steering and speed, but assumes the human can take over if it gets
   confused;
4. High automation - the software monitors the environment and drives the car,
   in some driving conditions, without assuming that a human can intervene. If
   it gets confused it stops at the side of the road;
5. Full automation - the software can do everything a human can.

*** How to Hack a Self-Driving Car

In 2010, Karl Koscher and colleagues got the attention of academics by showing
how to hack a late-model Ford.

Cars’ internal data communications use a *CAN bus*, which does not have strong
*authentication*, so an attacker who gets control of (say) the radio can
escalate this access to operate the door locks and the brakes.

In 2015, Charlie Miller and Chris Valasek got the attention of the press when
they hacked a Jeep Cherokee containing a volunteer journalist, over its mobile
phone link, slowed the vehicle down and drove it off the road. This compelled
Chrysler to recall 1.4m vehicles for a software patch, costing the company over
$1bn. This finally got the industry’s attention.

[...]

A reasonable worst-case scenario might see a state actor, or perhaps an
environmental activist group, trying to scare the public by causing thousands of
simultaneous road traffic accidents.

A remote exploit such as that on the Chrysler Jeep might already do this. The
*CAN bus* that most modern cars use for internal data communications trusts all
its nodes. If one of them is subverted it might be reprogrammed to transmit
continuously; such a ‘blethering idiot’, as it’s called, makes the whole bus
unusable. If this is the powertrain bus, the car becomes almost undriveable; the
driver will still have some steering control but without power assistance to
either steering or brakes. If the car is travelling at speed, there’s a serious
accident risk.

The possibility that a malicious actor could hack millions of cars causing tens
of thousands of road traffic accidents simultaneously is unacceptable, and such
vulnerabilities therefore have to be patched. But patching is expensive. The
average car might contain 50–100 electronic control units from 20 different
vendors, and the integration testing needed to get them to all work together
smoothly is expensive.

** AI/ML

Why it's hard? Pattern-matching tools can pick out not just real patterns in
human behaviour - sometimes unexpected ones - but false ones too.

Bayes’ theorem remains the same, but the pedantically cautious stats profs of
old are being replaced by AI evangelists. Some of the evangelists have exciting
sales pitches: around artificial general intelligence making robots
self-conscious, or a bright future in which we upload and become
immortal. Others see a darker future in which the robots take over.

Old-school AI researchers explained carefully to students that it’s all just
*pattern matching* and we have no idea at all about the nature of
*consciousness*.

The arrival of spam as the Internet opened up to the public in the mid-1990s
created a market for spam filters. Hand-crafted rules didn’t scale well enough
for large mail service providers, especially once botnets appeared and spam
became the majority of email, so spam filtering became a big application.

*** Attacks on ML systems

There are at least four types of attack on a *machine-learning system*.

1. You can poison the training data.

   If the model continues to train itself in use, then it can sometimes be
   simple to lead it astray.

   Tay was a chatbot released by Microsoft in March 2016 on Twitter; trolls
   immediately started teaching it to use racist and offensive language, and it
   was shut down after only 16 hours.

2. You can attack the *model’s integrity* in its *inference phase*, for example
   by causing it to give the wrong answer.

   In 2013, Christian Szegedy and colleagues found that the deep neural networks
   which had been found to classify images so well in 2012 were vulnerable to
   adversarial samples – images perturbed very slightly would be wildly
   misclassified. The idea is to choose a perturbation that maximises the
   model’s prediction error. It turns out that neural networks have plenty of
   such blind spots, which are related to the training data in non-obvious
   ways. The decision space is high-dimensional, which makes blind spots
   mathematically inevitable; and with neural networks the decision boundaries
   are convoluted, making them non-obvious.

   Researchers quickly came up with real-world adversarial examples, ranging
   from small stickers that would cause a car vision system to misread a 30mph
   speed sign as 60mph, to coloured spectacles that would cause a man wearing
   them to be misrecognised as a woman, or not recognised at all. In the world
   of malware detection, people found that non-linear classifiers such as SVM
   and deep neural networks were not actually harder to evade than linear
   classifiers provided you did it right.

3. Florian Tramèr and colleagues showed that you can attack the *model’s
   confidentiality* in the *inference phase*, by getting it to classify a number
   of probe inputs and building a successively better approximation.

   The result is often a good working imitation of the target model. As in the
   manufacture of real goods, a knock-off is often cheaper; big models can cost
   a lot to train from scratch. This approximation attack works not just with
   neural networks but also with other classifiers such as logistic regression
   and decision trees.

4. You can *deny service*, and one way is to choose samples that will cause the
   classifier to take as long as possible.

   Ilia Shumailov and colleagues found that one can often deny service by posing
   a conundrum to a classifier. Given a straight-through pipeline, as in a
   typical image-processing task, a confusing image can take 20% more time, but
   in more complex tasks such as natural language processing you can invoke
   exception handling and slow things down hundreds of times.

---

One fundamental problem is that once we start letting machine learning blur the
boundary between code and data, and systems become data-driven, people are going
to *game* them. This brings us to the thorny problem of the interaction of
machine learning and society.

** Privacy

Why it's hard? Due to the richness of human interaction in society.

Even if you don’t use Facebook at all, the traffic data on who contacted whom
gives a lot away to those who have access to it. In section 11.2.6 we discussed
research which suggested that as few as four Facebook likes enable a careful
observer to work out whether you’re straight or gay most of the time, and how
this observation led among other things to the Cambridge Analytica scandal,
where voters’ preferences were documented covertly and in detail.

** Elections

Why it's hard? First, because of the technical difficulty of counting votes in a
way that preserves both privacy and auditability. Second, because of the huge
variety of dirty tricks used by political players.

Elections remain one of the tough security engineering problems. While the
individual problems – such as *voter registration*, *vote casting*, *vote
counting*, *result aggregation* and *audit* – all have reasonably robust
solutions, putting them together into a robust system is nontrivial.

Computer systems for registering electors, recording votes and tallying them
have a number of properties which make them almost a pathological case for
robust design, implementation, testing and deployment.

First, the election date is immovable and, ready or not, the software must be
deployed then.

Second, different regions and countries have different requirements and they
change over time.

Third, in the long gap between elections, staff with experience move on and
know-how is lost.

Fourth, operating systems and other software must be updated to fix known
vulnerabilities, and updates can also break security in unforeseen ways; a
Windows update caused the EV2000 voting machine to highlight the last voter’s
choice to the next voter. Yet most voting machines in use in the USA are no
longer manufactured, so where are the updates to come from and how will they be
tested?

Finally, elections are high-stress events, which increases the likelihood of
mistakes.

** Off-topic

Tempest IDS - parte 1: [[https://www.sidechannel.blog/fortalecendo-sistemas-de-deteccao-de-intrusao-com-machine-learning-parte-1-de-5/][Fortalecendo Sistemas de Detecção de Intrusão com Machine
Learning]].

* Chapter 26 - Surveillance or Privacy?

In this chapter we explore the nexus of surveillance, censorship, forensics and
privacy.

** Surveillance

The 2010s saw a huge increase in technical surveillance, not just by government
but also by commercial firms monitoring our clickstream and location history in
order to target ads better.

+ In some countries, like the USA, law enforcement and intelligence agencies
  don't just get information from their own collection systems but use warrants
  to get it from firms like Google and Facebook too.
+ In others, like China, these firms are banned because they refused to give
  complete access to the authorities.
+ In others, like Iran and Syria, the police agencies just beat people's
  passwords out of them, or phish their friends, or hack their phones.

*** ISPs and CSPs

+ ISP: Internet Service Provider
+ CSP: Communications Service Providers (firms like Google and Yahoo)

Many countries now have laws requiring ISPs to help (the government), and the
usual way to do it at a large ISP is to have equipment already installed that
will send copies of packets of interest (or NetFlow records) to a separate
classified network. The FBI's system, DCSNet, is very slick - allowing agents
point-and-click access to traffic and content from participating phone
companies.

+ Information about which companies have been brought onboard is closely held,
  but smart bad guys use small ISPs.

The smartphone revolution has changed the natural control point from the ISP to
the CSP.

A modern criminal might get up, check his messages on Gmail or WhatsApp using
his home wifi, then get on a bus into town and do the same using his 3G or 4G
data connection, then perhaps use wifi at a Starbucks or a public library …  and
in none of these cases does a wiretap at the ISP tell anything much beyond the
fact that a particular service has been used. As the traffic to that
communications service is encrypted, the police have to serve paperwork on the
service to get anywhere. This is what led the FBI to set up the Prism system,
whereby intelligence agencies can get customer data from Google, Yahoo, Apple,
Microsoft, Facebook and others at the press of a button.  It is also what led
the UK, in its 2016 Investigatory Powers Act, to grant itself the power to order
any company to do anything it physically can in order to assist law-enforcement
of intelligence investigations. More and more countries are passing such laws,
which put the service providers in conflict with other countries’ laws.

*** The crypto wars

Many countries made laws in the mid-19th century banning the use of cryptography
in telegraph messages, and some even forbade the use of languages other than
those on an approved list. Prussia went as far as to require telegraph orpeators
to keep copies of the plaintext of all messages.

Sometimes the excuse was law enforcement - preventing people obtaining horse
race results or stock prices in advance of the 'official' transmissions - but
the real concern was national security.

[...] the authorities would try to steer applicants towards using weak
cryptography where possible, and where confronted with a more sophisticated user
would try to see to it that systems had a 'back door' (known in the trade as a
red thread) which would give access to traffic. Anyone who tried to sell decent
crypto domestically could be dissuaded by various means. If they were a large
company, they would be threatened with loss of government contracts; if a small
one, they could be strangled with red tape as they tried to get licenses and
product approvals.

* Chapter 27 - Secure Systems Development

If you're a working engineer, manager or consultant, paid to build or maintain a
system with some security assurance requirements, you will by now be looking for
a systematic way to go about it. This brings us to such topics as risk analysis,
system engineering methodology, and finally, how to manage a team to write
secure code.

[...] In his classic book "The Mythical Man-Month" he (Fred Brooks) describes
all the problems they struggled with, and his conclusion is that "there is no
silver bullet". There's no magic formula that makes an intrinsically hard job
easy.

** Risk Management

At the heart of both safety engineering and security engineering lie decisions
about priorities: how much to *spend* on protection against *what*. Risk
management must be done within a broader framework of managing all the risks to
an enterprise or indeed to a nation.

So what actually is a ~risk register~?

1. A common methodology, as used by the governing body of my university, is to
   draw up a list of things that could go wrong, giving them scores of 1 to 5
   for *seriousness* and for *probability of occurrence*, and multiplying these
   together to get a number between 1 and 25.
2. You then write down the measures you take to mitigate each of these risks,
   and have an argument in a risk committee about how well each risk is
   mitigated.
3. You then rank all the risks in order and assign one senior officer to be the
   owner of each of them.

*** National Risk Assessments

National Risk Assessments are somewhat similar: you rate each possible bad event
(pandemic, earthquake, forest fire, terrorist attack, ...) by how many people it
might kill (millions? thousands? dozens?) and then rate it for probability by
how many you expect each century.

You do the cost-benefit analysis and turn priorities into policy.

** Lessons from safety-critical systems

~Critical computer systems~ are those in which a certain class of failure is to
be avoided if at all possible. Depending on the class of failure, they may be:

+ safety-critical: flight controls and automatic braking systems;
+ business-critical: banking transactions;
+ security-critical;
+ critical to the environment.

*** Safety engineering methodologies

Safety engineering methodologies, like classical security engineering, tend to
work systematically from a safety analysis to a specification through to a
product, and assume you're building safety in from the start rather than trying
to retrofit it.

The usual procedure is to:

1. Identify hazards and assess risks;
2. Decide on a strategy to cope with them (avoidance, constraint, redundancy,
   ...).
3. Traces the hazards to hardware and software components which are thereby
   identified as critical;
4. Identify the operator procedures which are also critical and study the
   various applied psychology and operations research issues;
5. Set out the safety funcional requirements which specify what the safety
   mechanisms must do, and safety integrity requirements that specify the
   likelihood of a safety function being performed satisfactorily;
6. Finally, decide on a test plan.

The basic framework is set out in standards such as ISO 61508, a basic safety
framework for relatively simple programmable electronics such as the control
systems for chemical plants. This has been extended with more specialised
standards for particular industries, such as ISO 26262 for road vehicles.

This safety case will provide the evidence, if something does go wrong, that you
exercised due care.

*** Quantifying risks

The safety-critical systems community has a number of techniques for dealing
with failure and error rates.

+ Component failure rates can be measured statistically;
+ The number of bugs in software can be tracked by techniques described later;
+ There is lot of experience with the probability of operator error at different
  types of activity. The bible for human-factors engineering in safety-critical
  systems is ~James Reason's book "Human Error"~. Errors are rare in
  frequently-performed tasks at which the operator has developed some skill, and
  are more likely when operators are stressed and surprised.

Safety is like security in that it really has to be built in as a system is
developed, rather than retrofitted. The main difference between the two is in
the *failure model*. Safety deals with the effects of random failure, while in
security we assume a hostile opponent who can cause some of the components of
our system to fail at the least convenient time and in the most damaging way
possible. People are naturally more risk-averse in the presence of an
adversary.

A safety engineer will certify a critical flight-control system with an MTBF of
1 billion hours; a security engineer has to worry whether an adversary can force
the preconditions for that one-in-a-billion failure and crash the plane on
demand.

** Security incident and event management

You need an incident response plan for what you'll do when you learn of a
vulnerability or an attack. In the old days, vendors could take months to
respond with a new version of the product, and would often do nothing at all but
issue a warning (or even a denial). Nowadays, breach-notification laws in both
the USA and Europe oblige firms to disclose attacks where individuals' privacy
could have been compromised, and people expect that problems will be fixed
quickly.

Your plan needs four components:

+ Monitoring: Make sure you learn of vulnerabilities as soon as you can - and
  preferably no later than the bad guys (or the press) do. This means building a
  threat intelligence team.
+ Repair: Preparing an orchestrated response to anything from a vulnerability
  report to a major breach.
+ Distribution: You need to be able to deploy the patch rapidly. If all the
  software runs on your own servers, then it may be easy, but if it involves
  patching code in millions of consumer devices, then advance planning is
  needed.
+ Reassurance: Educate your CEO and main board directors in advance about the
  need to deal quickly and honestly with a security breach in order to keep
  confidence and limit damage, by giving them compelling examples of firms that
  did well and others that did badly.

* Chapter 28 - Assurance and Sustainability

This chapter focuses in some of the hardest parts of the security engineering
job.

First, there's the question of ~assurance~ - whether the system will work, and
how you're sure of this.

Second, there's its cousing ~compliance~ - how you satisfy other people about
this.

Third, there's ~sustainability~ - how long it will keep on working.

Many practical questions are linked to these.

+ How do you decide to ship the product?
+ How do you sell the security and safety case to your insurers?
+ How long are you going to have to maintain it, and at what cost?

** Free and Open-Source Software

Eric Raymond's influential analysis of the economics of open source software
(author of "The Cathedral and the Bazaar") suggests five criteria for whether a
product would be likely to benefit from an open source approach:

1. Where it is based on common engineering knowledge rather than trade secrets;
2. Where is is sensitive to failure;
3. Where it needs peer review for verification;
4. Where it is sufficiently business-critical that different users will
   cooperate in finding and removing bugs;
5. Where its economics include strong network effects.

** Process Assurance

In recent years less emphasis has come to be placed on assurance measures
focused on the product, such as testing, and more on process measures such as
who developed it and how. As anyone who’s done system development knows, some
programmers produce code with an order of magnitude fewer bugs than
others. There are also some organizations that produce much better code than
others. Capable firms try to hire good people, while good people prefer to work
for firms that value them and that hire kindred spirits.

** The Entanglement of Safety and Security

Governments regulate safety for many types of device from cars to railway
signals and from medical devices to toys.

As software finds its way into everything and everything gets connected to cloud
services, the nature of safety regulation is changing, from simple pre-market
safety testing to maintaining security and safety over a service lifetime of
years during which software will be patched regularly.

The EU is the leading safety regulator worldwide for dozens of industries, as
it's the largest market and cares more about safety than the US government
does. Officials wanted to know how this ecosystem would have to adapt to the
"Internet of Things" where vulnerabilities (whether old or new) may be remotely
exploited, and at scale. Many regulators who previously thought only in terms of
safety will have to start thinking of security as well.

For deaths, at least, you'd think we have decent statistics, but priorities are
modulated by public concern about different types of harm. [...] the public are
much more alarmed at a hundred people dying all at once in a plane crash than a
thousand people dying one at a time in medical device accidents. However, when
hackers showed they could go in over wifi and change the dose delivered by
several models of Hospira Symbiq infusion pump to a potentially fatal level, the
FDA issued a safety advisory telling hospitals to stop using it.

*** The Electronic Safety and Security of Cars

Autonomous cars:

+ Could terrorists hack cars and drive them into crowds?
+ Could they get the same result by projecting deceptive images on a building?
+ And if kids could use their phone to hail a car home from school, could
  someone hack it to abduct them?
+ And what about the ethics - if a self-driving car was about to crash and could
  choose between killing its one occupant or two pedestrians, what would it do?

*** Modernizing Safety and Security Regulation

The security engineer's task is to enable even vulnerable users to enjoy
reasonable protection agains a capable motivated opponent.

How to improve our current regulations to cope with the software environment
that is taking shape in our times?

+ Extending product liability law to services, and requiring the reporting of
  breaches and vulnerabilities not just to security agencies and privacy
  regulators but to other stakeholders too.

  Eventually we'll need laws regulating the use of car data in investigating
  accidents, particularly if there are disputes over liability when car
  autopilots cause fatal crashes.

  At present, the vendors hold the data close and it takes vigorous litigation
  to get hold of it. Without data we won't be able to build a learning system
  and improve overall.
+ Vendors should have to self-certify that products can be patched if need be.

** Sustainability

The problem our report identified as the most serious in the long term was that
products are becoming much less static. As security and safety vulnerabilities
are patches, regulators will have to deal with a moving target. Automobile
mechanisms will need security testing as well as safety testing, and also means
of dealing with updates.

+ Should we allow an old car that does not support updates to run in the same
  road that other cars are running?

In this topic we have another aspect to take into consideration. Planned
obsolescence was already a hot political topic as green parties increased their
vote share across Europe. Lightbulbs used to last longer; the bicentennial light
has been burning at Livermore since 1901. In 1924 a cartel of GE, Osram and
Philips agreed to reduce average bulb lifetimes from 2500h to 1000h, and this
behaviour has been followed by many industries since.

*** The Sales of Goods Directive

From 2021 on, firms selling goods "with digital elements" must maintain those
elements for a reasonable service life. The wording is designed to cover
software in the goods themselves, online services to which the goods are
connected, and apps that may communicate with the goods either via the services
or directly. They must be maintained for a minimum of *two* years after sale,
and for a longer period if that is a reasonable expectation of the customer.

Existing regulations require vendors of durables such as cars and washing
machines to keep supplying spares for at least ten years.

** Summary

In the old days, the big question in a security engineering project was how you
know when you’re done. All sorts of evaluation and assurance methodologies were
devised to help. Now the world is different. We’re never done, and nobody who
says they are done should be trusted.

* Chapter 29 - Beyond "Computer Says No"

So, what next? By way of a conclusion to this book, I'd like to highlight three
things.

1. First, complexity. Computer science has spent seventy years devising an
   impressive array of tools to manage technical complexity, but we're now
   coming up hard against social complexity. We can program cars to drive
   themselves fairly well on the freeway or in the desert, but we can't cope
   with cluttered city streets with all those unpredictable people. We can
   encrypt messages or strip people's names from databases but we can't stop
   social structure showing through. And bullying people has its limits;
   "computer says no" is a fast way to lose customers. It's not enough to study
   how a computer system cam interact with a human; we need to figure out how it
   can work with many interacting humans.
2. Second, sustainability. As we put software in everything and connect
   everything online, we have to patch the software and maintain the
   servers. With durable goods like cars, pacemakers and electricity
   substations, we may have to maintain software for twenty or even forty
   years. We have no real idea how to do that, and if we don't crack it then our
   automation will be bad news for out planet's future. So-called "smart"
   devices are often just things that have to be thrown away sooner, when
   "computer says no".
3. Third, politics. Security is not a scalar, but a relationship. It's not some
   kind of magic fairy dust you sprinkle on systems, but about how these systems
   exercise power. Who loses and who gains when "computer says no"? Does the
   social-network user get privacy, or does the advertiser get access? How is it
   used to turn money into political power? And if people want public goods such
   as a dependable Internet or a low rate of cybercrime, how can these be
   provided in a global world?
